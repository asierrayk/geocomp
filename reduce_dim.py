# -*- coding: utf-8 -*-

from __future__ import division
import numpy as np
from scipy.linalg import eigh


class LDA:

    """
    Class which implements Fisherâ€™s discriminant for
    multiple classes.
    First trains with given points belonging to known
    classes.
    Then classifies using the values obtained during
    the training.

    Notes
    -----
    LDA stands for Lineal Discriminant Analysis.
    For further information, see Bishop, section 4.1.6

    Methods
    -------
    fit
    transform

    Atributes
    ---------
    W_LDA : projection matrix
    mean_train : mean of the total training data set

    """

    def __init__(self):
        self.W_LDA = None
        self.mean_train = None

    def fit(self, X_train, y_train, reduced_dim):
        """
        Create projection matrix. For that, use given
        training data: X_train and y_train.

        Notes
        -----
        After applying the projection matrix to the
        yet to be classified points, they will change
        their dimension to the given reduced_dim.

        Parameters
        ----------
        X_train :
            array of dimension (N,D) which contains
            N points with known classes.
        y_train :
            array that tells which classes the points
            from X_train belong to.
        reduced_dim :
            integer number (less than number of classes - 1)
            used for dimension reduction.

        Examples
        --------
        >>> 

        """

        # Save the mean value for further use
        self.mean_train = np.mean(X_train, axis=0)

        classes = np.unique(y_train)
        N, D = X_train.shape

        # S within
        S_w = np.zeros((D, D))
        for k in classes:
            X_k = X_train[y_train == k, :]
            N_k = X_k.shape[0]

            S_w += np.cov(X_k, rowvar=0, bias=1) * N_k

        # S between
        S_t = np.cov(X_train, rowvar=0, bias=1) * N
        S_b = S_t - S_w

        # Select eigenvectors whose eigenvals are the reduced_dim greatest
        eigh_vectors = eigh(S_b, S_w, eigvals=(D - reduced_dim, D - 1))[1]
        w = (eigh_vectors.T)[::-1] # descending order
        self.W_LDA = w.T

    def transform(self, X):
        """
        Project given array of points to the space generated by the
        previously selected eigenvectors. Resulting points will be of
        dimension reduced_dim.

        Parameters
        ----------
        X :
            array of points we wish to project.

        Returns
        -------
        Array of points
            Result of projecting points in X with projection matrix W_LDA

        Examples
        --------
        >>>

        """

        X_red = (X - self.mean_train).dot(self.W_LDA)
        return np.asarray(X_red)


class PCA:

    def __init__(self):
        self.W_PCA = None
        self.mean_train = None

    def fit(self, X_train, reduced_dim):
        N, D = X_train.shape
        self.mean_train = np.mean(X_train, axis=0)

        S_t = np.cov(X_train, rowvar=0, bias=1)
        eigh_vectors = eigh(S_t, eigvals=(D - reduced_dim, D - 1))[1]
        w = (eigh_vectors.T)[::-1]
        self.W_PCA = w.T

    def transform(self, X):
        X_red = (X - self.mean_train).dot(self.W_PCA)
        return X_red
